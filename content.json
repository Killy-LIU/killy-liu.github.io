[{"title":"","date":"2017-05-24T07:55:22.000Z","path":"2017/05/24/Ubuntu虚拟机实现两结点SSH免密码通信/","text":"title: Ubuntu虚拟机实现两结点SSH免密码通信date: 2017-05-11 15:55:22tags: - 技术 - SSH - Ubuntu虚拟机 实现功能： 创建两台Ubuntu VM 16.04 虚拟机，并更改hostname 做结点的hostname到IP的映射 并实现SSH通信 在虚拟机上部署服务，查看应用效果 准备工作 &emsp;&ensp;这里创建了两台ubuntu VM 16.04 虚拟机进行操作。 &emsp;&ensp;Server1: 192.168.113.143 &emsp;&ensp;Server2: 192.168.113.144 设置主机名hostname &emsp;&ensp;编辑hostname文件，设置主机名。这里为了识别方便，给两个服务器结点分别设为server1, server2。1$ sudo gedit /etc/hostname &emsp;&ensp;重启虚拟机，便可以看到服务器的主机名为server1，server2。 设置IP主机映射 &emsp;&ensp;编辑hosts文件并设置hosts与IP的映射关系：1$ sudo gedit /etc/hosts 在文件中添加信息：12192.168.113.143 server1192.168.113.144 server2 SSH配置设置SSH keygen &emsp;&ensp;为每个服务器设置keygen：1$ ssh-keygen –t rsa 拷贝SSH KEY &emsp;&ensp;将公秘钥拷贝给对方服务器：1$ ssh-copy-id server2 注：如果这里我们之前做过拷贝，则需要用-f：1$ ssh-copy-id –f server2 使用SSH登录到对方服务器 &emsp;&ensp;这里举例用server2登录到server1:1$ ssh server1 Tomcat配置jdk配置 &emsp;&ensp;使用远程拷贝的方式将电脑本地的jdk包传到远程服务器上，并解压做相关配置。 &emsp;&ensp;编辑profile文件，做JAVA_HOME配置：123456789$ vim /etc/profileJAVA_HOME=/usr/local/jdk1.8.0_131JRE_HOME=$JAVA_HOME/jrePATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binCLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport JAVA_HOME JRE_HOME PATH CLASSPATH$ source /etc/profile &emsp;&ensp;使用java -version查看现在的jdk信息：1$ java -version Tomcat配置 &emsp;&ensp;下载并解压Tomcat。 &emsp;&ensp;进入Tomcat的安装地址并启动Tomcat1$ bin/startup.sh war包运行 &emsp;&ensp;这里我将自己的项目打包成war包，并且放在tomcat下的webapps 文件夹下： &emsp;&ensp;重新运行tomcat，即可看到自己的项目:1$ bin/startup.sh","tags":[]},{"title":"Java实现Hadoop下词配对Wordcount计数","date":"2017-05-15T09:01:00.000Z","path":"2017/05/15/Java实现Hadoop下词配对Wordcount计数/","text":"需求&emsp;&emsp;使用Hadop实现 Mapper/Reducer，对一个文档中的每一行的单词进行词配对计数，要求去标点符号，将大写符号统一转化成为小写单词。 &emsp;&emsp;举例说明，最初的文档为： “a a, A ba b c 则处理后的结果为： (a a) 2(a b) 2(a c) 1(b a) 4(b c) 1(c a) 1(c b) 1 实现过程开启hadoop进入Hadoop所在的文件夹并执行启动语句：1$ sbin/start-all.sh jar包处理将编码程序打包成jar包进行处理 HDFS文件设置使用hdfs创建文件夹，并将input文件放在hdfs文件夹下：123$ bin/hdfs dfs -mkdir -p /wordcount1/input$ bin/hdfs dfs -put /Users/liuqi/Desktop/input.txt /wordcount1/input$ bin/hdfs dfs -ls /wordcount1/input mapreduce程序运行mapreduce程序：1$ bin/hadoop jar /Users/liuqi/Desktop/wordcount.jar WordCount /wordcount1/input /wordcount1/output 注：如果中间有错，则删除对应文件重新进行操作：1$ bin/hdfs dfs -rm -r /wordcount1/input 查询结果查看计数的结果：12$ bin/hdfs dfs -ls /wordcount1/output$ bin/hdfs dfs -cat /wordcount1/output/part-00000 将结果保存在本地：1$ bin/hdfs dfs -getmerge /wordcount1/output /Users/liuqi/Desktop/wordcount1/ 附加代码注：这里只是显示java代码，整个工程去我的CSDN博客进行下载： http://download.csdn.net/detail/u012842255/9851124 WordCount.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135import java.io.IOException; import java.util.Iterator; import java.util.StringTokenizer; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IntWritable; import org.apache.hadoop.io.LongWritable; import org.apache.hadoop.io.Text; import org.apache.hadoop.mapred.FileInputFormat; import org.apache.hadoop.mapred.FileOutputFormat; import org.apache.hadoop.mapred.JobClient; import org.apache.hadoop.mapred.JobConf; import org.apache.hadoop.mapred.MapReduceBase; import org.apache.hadoop.mapred.Mapper; import org.apache.hadoop.mapred.OutputCollector; import org.apache.hadoop.mapred.Reducer; import org.apache.hadoop.mapred.Reporter; import org.apache.hadoop.mapred.TextInputFormat; import org.apache.hadoop.mapred.TextOutputFormat; /** * * WordCount * @author 刘琦 */ public class WordCount &#123; /** * MapReduceBase类:实现了Mapper和Reducer接口的基类（其中的方法只是实现接口，而未作任何事情） * Mapper接口： * WritableComparable接口：实现WritableComparable的类可以相互比较。所有被用作key的类应该实现此接口。 * Reporter 则可用于报告整个应用的运行进度，本例中未使用。 * */ public static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; /** * LongWritable, IntWritable, Text 均是 Hadoop 中实现的用于封装 Java 数据类型的类，这些类实现了WritableComparable接口， * 都能够被串行化从而便于在分布式环境中进行数据交换，你可以将它们分别视为long,int,String 的替代品。 */ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); /** * Mapper接口中的map方法： * void map(K1 key, V1 value, OutputCollector&lt;K2,V2&gt; output, Reporter reporter) * 映射一个单个的输入k/v对到一个中间的k/v对 * 输出对不需要和输入对是相同的类型，输入对可以映射到0个或多个输出对。 * OutputCollector接口：收集Mapper和Reducer输出的&lt;k,v&gt;对。 * OutputCollector接口的collect(k, v)方法:增加一个(k,v)对到output */ public void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; //变成小写单词，并且去各种符号 String line = value.toString().toLowerCase().replaceAll(&quot;[\\\\d\\\\pP\\\\p&#123;Punct&#125;]&quot;, &quot;&quot;); StringTokenizer tokenizer = new StringTokenizer(line); StringTokenizer tokenizer2 = new StringTokenizer(line); String[][] result = new String[tokenizer.countTokens()][2]; int k = 0; while (tokenizer.hasMoreTokens()) &#123; result[k][0] = tokenizer.nextToken(); result[k][1] = &quot;0&quot;; k++; &#125; //对每一行的单词进行处理 for(int i = 0; i &lt; tokenizer2.countTokens() ; i++)&#123; for(int j = 0; j &lt; tokenizer2.countTokens() ; j++)&#123; if(i == j)&#123; continue; &#125;else if (result[i][1].equals(&quot;1&quot;))&#123; //这个词之前出现过了，这里只统计它之后还有没有相同的数据,后来发现不需要这一步了，相同的只计算一次就好 if(i&lt;j &amp;&amp; result[i][0].equals(result[j][0]))&#123; result[j][1] = &quot;1&quot;; word.set(&quot;(&quot; + result[i][0] + &quot; &quot; + result[j][0] + &quot;)&quot;);// output.collect(word, one); &#125; &#125;else&#123; //这个词之前没有出现过 if (!result[i][0].equals(result[j][0]))&#123; //普通操作 word.set(&quot;(&quot; + result[i][0] + &quot; &quot; + result[j][0] + &quot;)&quot;); output.collect(word, one); &#125;else&#123; //说明两个单词是一样的,并且这个单词之前没有统计过 result[j][1] = &quot;1&quot;; word.set(&quot;(&quot; + result[i][0] + &quot; &quot; + result[j][0] + &quot;)&quot;); output.collect(word, one); &#125; &#125; &#125; &#125; &#125; &#125; public static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; public void reduce(Text key, Iterator&lt;IntWritable&gt; values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException &#123; int sum = 0; while (values.hasNext()) &#123; sum += values.next().get(); &#125; output.collect(key, new IntWritable(sum)); &#125; &#125; public static void main(String[] args) throws Exception &#123; /** * JobConf：map/reduce的job配置类，向hadoop框架描述map-reduce执行的工作 * 构造方法：JobConf()、JobConf(Class exampleClass)、JobConf(Configuration conf)等 */ JobConf conf = new JobConf(WordCount.class); conf.setJobName(&quot;wordcount&quot;); //设置一个用户定义的job名称 conf.setOutputKeyClass(Text.class); //为job的输出数据设置Key类 conf.setOutputValueClass(IntWritable.class); //为job输出设置value类 conf.setMapperClass(Map.class); //为job设置Mapper类 conf.setCombinerClass(Reduce.class); //为job设置Combiner类 conf.setReducerClass(Reduce.class); //为job设置Reduce类 conf.setInputFormat(TextInputFormat.class); //为map-reduce任务设置InputFormat实现类 conf.setOutputFormat(TextOutputFormat.class); //为map-reduce任务设置OutputFormat实现类 /** * InputFormat描述map-reduce中对job的输入定义 * setInputPaths():为map-reduce job设置路径数组作为输入列表 * setInputPath()：为map-reduce job设置路径数组作为输出列表 */ FileInputFormat.setInputPaths(conf, new Path(args[0])); FileOutputFormat.setOutputPath(conf, new Path(args[1])); JobClient.runJob(conf); //运行一个job &#125; &#125;","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"SSH","slug":"SSH","permalink":"http://blog.killyliu.com/tags/SSH/"},{"name":"Ubuntu虚拟机","slug":"Ubuntu虚拟机","permalink":"http://blog.killyliu.com/tags/Ubuntu虚拟机/"}]},{"title":"阿里云Ubuntu服务器实现两结点SSH免密码通信","date":"2017-05-10T06:55:00.000Z","path":"2017/05/10/阿里云Ubuntu服务器实现两结点SSH免密码通信/","text":"实现功能： 阿里云上创建两个结点，更改hostname 做结点的hostname到IP的映射 并实现SSH通信 在虚拟机上部署服务，查看应用效果 准备工作 &emsp;&ensp;这里在阿里云上创建了两个实例，这里我的两个服务器分别是：使用学生套餐9.9租用了一个服务器，使用按时按流量计费的方式租用了另一台服务器。 &emsp;&ensp;都创建为：ubuntu VM 16.04 &emsp;&ensp;Server1: 47.94.95.40 / 172.17.78.48 &emsp;&ensp;Server2: 47.52.107.97 / 172.31.162.43 设置主机名hostname &emsp;&ensp;编辑hostname文件，设置主机名。这里为了识别方便，给两个服务器结点分别设为server1, server2。1$ vim /etc/hostname &emsp;&ensp;重启虚拟机，便可以看到服务器的主机名为server1，server2。1$ shutdown –r now 设置IP主机映射 &emsp;&ensp;编辑hosts文件并设置hosts与IP的映射关系：1vim /etc/hosts 在文件中添加信息：1247.94.95.40 server147.52.107.97 server2 SSH配置设置SSH keygen &emsp;&ensp;首先，要确认服务器已开启密码权利(password authentication)1$ vim /etc/ssh/sshd_config &emsp;&ensp;确认passwordAuthentication 是yes，然后重新加载。1$ /etc/init.d/sshd reload &emsp;&ensp;然后为每个服务器设置keygen：1$ ssh-keygen 拷贝SSH KEY &emsp;&ensp;将公秘钥拷贝给对方服务器：1$ ssh-copy-id root@server2 使用SSH登录到对方服务器 &emsp;&ensp;这里举例用server2登录到server1:1$ ssh server1 Tomcat配置jdk配置 &emsp;&ensp;使用远程拷贝的方式将电脑本地的jdk包传到远程服务器上，并解压做相关配置。1$ scp /Users/liuqi/Downloads/jdk-8u131-linux-x64.tar root@47.94.95.40:/usr/local &emsp;&ensp;编辑profile文件，做JAVA_HOME配置：123456789$ vim /etc/profileJAVA_HOME=/usr/local/jdk1.8.0_131JRE_HOME=$JAVA_HOME/jrePATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binCLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport JAVA_HOME JRE_HOME PATH CLASSPATH$ source /etc/profile &emsp;&ensp;使用java -version查看现在的jdk信息：1$ java -version Tomcat配置 &emsp;&ensp;下载并解压Tomcat:123$ wget http://mirrors.hust.edu.cn/apache/tomcat/tomcat-7/v7.0.77/bin/apache-tomcat-7.0.77.tar.gz$ mv apache-tomcat-7.0.77.tar.gz /usr/local$ tar -xvzf /usr/local/apache-tomcat-7.0.77.tar.gz &emsp;&ensp;进入Tomcat的安装地址并启动Tomcat1$ bin/startup.sh war包运行 &emsp;&ensp;这里我将自己的项目打包成war包，并且放在tomcat下的webapps 文件夹下，重新运行tomcat，即可看到自己的项目：","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"SSH","slug":"SSH","permalink":"http://blog.killyliu.com/tags/SSH/"},{"name":"阿里云服务器","slug":"阿里云服务器","permalink":"http://blog.killyliu.com/tags/阿里云服务器/"}]},{"title":"挖掘模型(3)--Mahout建基于用户的推荐模型","date":"2017-05-03T15:52:31.000Z","path":"2017/05/03/挖掘模型-3-Mahout建基于用户的推荐模型/","text":"这里我们用mahout建简单的基于用户的推荐。 前提条件 已经安装好Eclipse环境 下载完成mahout配置包 工程环境打开eclipse，新建一个maven项目 使用默认工作环境 选择快速开始 设置基本信息：Group ID：com.recommenderExampleArtifact ID：MahoutRecommender默认版本：0.0.1-SNAPSHOT包：com.recommenderExample.MahoutRecommender 创建完成 项目右键-&gt;新建-&gt;文件夹 分别创建：data：用于存放数据集lib：用于引入其他数据库(主要为mahout数据库) 将下载的mahout压缩包解压，复制jar包到Lib文件夹下 右键点击项目-&gt;配置环境 点击添加jar包，添加刚引入的包 数据集Mahout推荐算法是希望将用户与项之间的关系作为输入进行计算，这里采用最简单的txt文本方式保存数据。我们用”dataset.csv”来保存数据，分别代表userID,itemID,value。数据集： 1,10,1.01,11,2.01,12,5.01,13,5.01,14,5.01,15,4.01,16,5.01,17,1.01,18,5.02,10,1.02,11,2.02,15,5.02,16,4.52,17,1.02,18,5.03,11,2.53,12,4.53,13,4.03,14,3.03,15,3.53,16,4.53,17,4.03,18,5.04,10,5.04,11,5.04,12,5.04,13,0.04,14,2.04,15,3.04,16,1.04,17,4.04,18,1.0 点击data文件夹(右键)-&gt;新建-&gt;文件，命名 dataset.csv 将数据文件拖到面板中，复制上面的数据 创建基于用户的推荐在App类中创建数据模型：1DataModel model = new FileDataModel(new File(&quot;/path/to/dataset.csv&quot;)); 我们根据其他相似品味用户的信息来推断特定用户的推荐，用相互作用关系来推断彼此之间的联系：1UserSimilarity similarity = new PearsonCorrelationSimilarity(model); 定义相似的用户，超过0.1的可能性则认为是相似的：1UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.1, similarity, model); 根据数据模型、相似信息、相似用户来创建推荐：1UserBasedRecommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity); 显示推荐信息：1234List recommendations = recommender.recommend(2, 3);for (RecommendedItem recommendation : recommendations) &#123; System.out.println(recommendation);&#125; 运行：可以看出：用户2 中 item12的阅读值为4.8328104，item13的阅读值为4.6656213，item13的阅读值为4.331242。用户2最可能购买的是item12，13，14。 对于警告：可以添加slf4j-nop-1.7.7.jar解决。 评估我们区分数据集为两种：90% trainingset10% testset创建新类：EvaluateRecommender 创建实现RecommenderBuilder接口的类MyRecommenderBuilder:12345678class myRecommenerBuilder implements RecommenderBuilder&#123; public Recommender buildRecommender(DataModel dataModel) throws TasteException &#123; UserSimilarity similarity = new PearsonCorrelationSimilarity(dataModel); UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.1, similarity, dataModel); return new GenericUserBasedRecommender(dataModel, neighborhood, similarity); &#125; &#125; 测试代码：12345DataModel model = new FileDataModel(new File(&quot;data/dataset.csv&quot;)); RecommenderEvaluator evaluator = new AverageAbsoluteDifferenceRecommenderEvaluator(); RecommenderBuilder builder = new myRecommenderBuilder(); double result = evaluator.evaluate(builder, null, model, 0.9, 1.0); System.out.println(result); 进行测试(每次运行结果不同，值越低越好)：","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"http://blog.killyliu.com/tags/数据挖掘/"}]},{"title":"挖掘模型(2)--Oracle建挖掘模型","date":"2017-05-02T15:50:41.000Z","path":"2017/05/02/挖掘模型-2-Oracle建挖掘模型/","text":"场景：寻找可能购买保险的潜在客户。 前提条件 下载并安装完成Oracle 11g (R2)企业版 下载Oracle管理工具Oracle SQL Developer3.0或者更新版(windows下使用) 系统用户(SYS)已经建好：密码，数据库端口号，数据库服务ID 数据挖掘账号创建与连接创建数据挖掘用户账号双击 sqldeveloper.exe 打开管理工具 点击连接(右键)-&gt;新建连接 填写信息： 连接名：admin用户名：sys口令：自己设置的Oracle数据库密码连接类型：基本角色：SYSDBA主机名：localhost端口号：自己设置的端口号，默认1521SID：自己设置的服务ID 点击测试成功后，点击连接。 创建数据挖掘的账号：点开admin账号，点击其他用户(右键)-&gt;创建用户 填写信息： 用户名：dduser新口令：用户密码确认新口令：用户密码默认表空间：USERS临时表空间：TEMP 点击授予的权限-&gt;设置连接 点击限额-&gt;用户USER设为无限制，点击应用。 为数据挖掘账户创建连接点击连接(右键)-&gt;新建连接 填写信息： 连接名：dduser用户名：dduser口令：自己设置的密码连接类型：基本角色：默认主机名：localhost端口号：自己设置的端口号，默认1521SID：自己设置的服务ID 点击连接： 下载数据挖掘知识库点击查看(V)-&gt;数据挖掘-&gt;数据挖掘连接 可以看到已经跳转到数据挖掘页面 点击连接(右键)-&gt;添加连接 建立与admin和dduser的连接 双击dduser，对于权限提醒，选择配置权限(是) 出现连接信息，输入密码，点击确定。 点击开始按钮，运行安装任务 创建数据挖掘项目在数据挖掘一栏，点击dduser(右键)-&gt;新建项目 填写项目名 ：ABC Insurance 点击确定，结果如下： 构建数据挖掘工作流这里，我们用挖掘模型预测已有客户中可能会购买保险的人。 创建工作流并添加数据源点击ABCInsurance(右键)-&gt;新建工作流 填写工作流名称：Targeting Best Customers 会出现工作流信息，右面有组件信息 添加数据源结点，拖到工作流页面中。 定义数据源：选择可用表：CUST_INSUR_LTV_SAMPLE，下一步选择列：这里选择所有列，点击完成。 检验元数据添加浏览数据结点注：右上角有感叹号说明还没有完成整个步骤 右键点击数据源结点(INSUR_CUST_LTV_SAMPLE)，选择连接，点击浏览数据 双击 浏览数据结点，分组方式选择BUY_INSURANCE，点击确定 点击浏览数据(右键)-&gt;运行 运行完成的效果如下： 右键浏览数据，选择查看数据： 显示浏览数据 注：之前GROUP BY可以选择不同的类型：Histogram, Distinct Values, Mode, Average, Min and Max value, Standard Deviation, Variance, Skewness, and Kurtosis，可以根据需要做不同的分析。 创建分类模型选择分类模型： 建立从数据元到分类构建的连接 目标：BUY_INSURANCE案例ID：CUSTOMER_ID双击CLAS_SVM_31_3,，进入高级模式设置，核函数选择线性。 构建模型(“training” a model)点击类构建(右键)-&gt;转到属性 在测试一栏中将拆分供测试设置为50 运行类构建 在属性中可以看到所有模型构建成功 比较模型点击类构建(右键)-&gt;比较测试结果 四个模型的基本比较： 选择提升按钮查看模型： 选择性能矩阵比较(选择具体的模型可以查看比较)： 选择和测试具体模型这里我们以决策树来举例。点击类构建(右键)-&gt;查看模型-&gt;CLAS_DT_1_3 显示节点具体信息(比例最好选择100%) 点击节点可以查看到具体信息： 应用模型添加数据源选择表：CUST_INSUR_LTV_SAMPLE重命名：CUST_INSUR_LTV_APPLY 在评估与应用中，选择 应用重命名：Apply Model 在类构建与应用(Apply Model)之间添加链接 在表(CUST_INSUR_LTV_APPLY) 与应用(Apply Model)之间添加链接 这里做简单的输出调整：点击应用(右键)-&gt;编辑 设置预测节点输出：这里可以看到已有的预测列 我们需要再添加一列CUSTOMER_ID：点击 附加输出-&gt;添加按钮-&gt;选择CUSTOMER_ID 点击确定之后，运行应用模型： 最后，每个图标的右上角都会有一个小勾，表明运行成功 将预测结果保存在数据库表中(可选)将创建表或试图 拖到界面中 建立从应用模型(Apply Model)到输出节点(OUTPUT)的连接 点击 输出 节点-&gt;编辑 设置表名：DT_PREDICTIONS 确定后点击表节点（DT_PREDICTIONS）右键运行 运行成功后，右键表节点（DT_PREDICTIONS）-&gt;查看视图 点击排序，选择CLAS_DT_1_3_PROB降序形式 点击应用排序并查看结果","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"http://blog.killyliu.com/tags/数据挖掘/"}]},{"title":"挖掘模型(1)--SqlServer2012建挖掘模型","date":"2017-05-01T15:46:13.000Z","path":"2017/05/01/挖掘模型-1-SqlServer2012建挖掘模型/","text":"&emsp;&emsp;Microsoft SQL Server提供了集成的数据挖掘建模分析环境，这里我们用SQL Server官网提供的数据创建模型并用于分析顾客购车行为，从而预测潜在购车客户。 前提条件安装：Microsoft SQL Server 2012 （注：最好英文版，中文版有些地方可能会出错）安装：多维模式下的Microsoft SQL Server 分析服务数据库：采用官网的样例数据库 http://go.microsoft.com/fwlink/?LinkId=88417 准备分析服务数据库&emsp;&emsp;使用SQL Server建立商业智能应用程序（ business intelligence application）进行建模分析。用数据工具(SQL Server Data Tools (SSDT))建立分析服务项目（SQL Server Analysis Services project），之后建立一或多个数据源（data sources），然后定义元数据视图（data source view）。 创建分析服务项目(Analysis Services Project)打开SQL Server数据工具(SQL Server Data Tools (SSDT)) 注意：要确认是商业智能项目模式（Business Intelligence Projects）。更改步骤：工具-&gt;导入和导出设置。一直到下一步，可以看到环境为 商业智能集合。 点击 文件-&gt;新建-&gt;项目 选择分析服务多维和数据挖掘项目（Analysis Services Multidimensional and Data Mining Project），项目起名为CustomerDataMining。 确认项目部署的服务名：点击项目(右键)-&gt;属性-&gt;部署-&gt;服务器为localhost。 创建数据源(Data Source)数据源文件夹(右键)-&gt;新建数据源 跳过欢迎页进行下一步-&gt;新建 点击服务器名，选择自己的服务器(若为空则手动输入localhost)-&gt;刷新-&gt;选择数据库(这里我们从外部导入之下下载的样例数据库)-&gt;确定 选择服务账户 数据源起名Adventure Works DW 2012，点击完成。 创建数据源视图(Data Source View)通过数据源视图，我们可以选择项目所需要的数据，建立表之间的关系，在不修改原来的数据的情况下修改数据的结构。 点击数据源视图(右键)-&gt;新建数据源视图 选择已有数据源-&gt;下一步 选择需要的表或视图，这里我们选择：ProspectiveBuyer (dbo) – 可能购车者信息vTargetMail (dbo) – 曾经购车者信息 点击下一步，设置视图名 Targeted Mailing，点击完成。 创建目标邮件结构创建目标邮件挖掘模型结构(Targeted Mailing Mining Model Structure)点击挖掘结构(右键)-&gt;新建挖掘结构 下一步-&gt;从已有的数据仓库选择-&gt;数据挖掘结构，我们选择决策树 选择数据源： 选择模型信息，我们最少需要一个预测列，一个输入列和一个关键值列。 预测列：• BikeBuyer关键值：• CustomerKey输入列：• Age• CommuteDistance• EnglishEducation• EnglishOccupation• Gender• GeographyKey• HouseOwnerFlag• MaritalStatus• NumberCarsOwned• NumberChildrenAtHome • Region• TotalChildren• YearlyIncome其他分析列：• AddressLine1• AddressLine2• DateFirstPurchase • EmailAddress• FirstName• LastName 确认数据类型，内容类型(Content and Data Type)点击检测按钮，对信息类型进行基本检测，点击完成。注：这里GeographyKey 是文本，不然可能会有标识符等不识别错误。 确认测试数据集结构(Testing Data Set for the Structure)这里设置测试比例为30%，测试集中最大数量为1000。 挖掘数据结构名：Targeted Mailing挖掘模型名：TM_Decision_Tree选择允许通过，点击完成。 添加·处理模型点击挖掘模型页面，我们可以看到之前建立的决策树模型，这里我们再建立两个模型，模型处理这里省略了。 聚类分析挖掘模型点击结构(右键)-&gt;新建挖掘模型 模型名 TM_Clustering，选择聚类分析算法。 朴素贝叶斯挖掘模型点击结构(右键)-&gt;新建挖掘模型模型名 TM_NaiveBayes，选择朴素贝叶斯算法。 模型探索由于挖掘模型的结果是复杂的，因而我们采用图形等简易的方式进行展示更加直观。 决策树模型点击查看挖掘模型视图，点击部署，运行。运行报错 原因是没有设置数据库的用户名。打开管理工具，新建登录，登录名为报错内容的ODBC连接错误用户名。 重新部署。 显示等级默认为3，这里改为4。背景值改为1，（这里1代表曾经购买过车，0代表未曾购买过车） 点击结点(右键) -&gt;钻取-&gt;仅模型/模型和结构 聚类分析模型挖掘模型部分选择聚类分析模型，选择Microsoft集群视图( Microsoft Cluster Viewer)，阴影变量处选购车者，状态选1。 点击节点(右键)可以进行重命名，这里将浅色节点命名Bike Buyers Low，深色节点为Bike Buyers High。 点击集群配置文件，设置直方图为5，查看不同因素的影响。 点击集群识别标签，设置集群1为Bike Buyers High，集群2为Bike Buyers Low。 朴素贝叶斯模型选择朴素贝叶斯模型，同样进行相关分析。 测试模型用梯度图测试准确性选择输入数据集，模型，预测列，值。 点击梯度图： 测试过滤模型之前我们已经对比得到决策树的准确性相对最高，这里用于测不同性别人购车对比。在挖掘模型页面新建两个模型：TM_Decision_Tree_Male，TM_Decision_Tree_Female。 点击决策树右键，添加模型筛选器 将新建的两个模型点右键进行处理 对两个模型都点击挖掘模型视图，设置背景为1，等级为3。 设置精确度 查看不同性别对比 进行预测创建预测在挖掘模型中，点击选择模型，选择决策树模型。 在测试表中选择 ProspectiveBuyer (dbo)更改信息 在解决方案资源管理器中，数据源视图右键，点击视图设计器 右键表ProspectiveBuyer，新建命名计算，输入信息。列名 calcAge，描述 Calculate age based on birthdate，表达式 DATEDIFF(YYYY,[BirthDate],getdate()) 在挖掘模型中，重新修改连接，年龄选择ProspectiveBuyer.calcAge。 source选择Prediction Function，field选择PredictProbability，alias选择Probability of result，将Bike Buyer拖到Criteria/Argument。 点击查询结果 钻取数据视图在挖掘模型视图中，选择结点（例age &gt;= 43 and &lt; 50）右键钻取，模型和结构列，查看钻取结果。","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"http://blog.killyliu.com/tags/数据挖掘/"}]},{"title":"人脸识别(4)--Python3.6+dlib19.4识别实例","date":"2017-04-20T05:29:14.000Z","path":"2017/04/20/人脸识别-4-Python3-6-dlib19-4识别实例/","text":"前提条件：确保python+dlib环境已经搭建成功，(搭建步骤可以参考上一篇博客)。 生成方形框识别人脸官网代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/usr/bin/python# The contents of this file are in the public domain. See LICENSE_FOR_EXAMPLE_PROGRAMS.txt## This example program shows how to find frontal human faces in an image. In# particular, it shows how you can take a list of images from the command# line and display each on the screen with red boxes overlaid on each human# face.## The examples/faces folder contains some jpg images of people. You can run# this program on them and see the detections by executing the# following command:# ./face_detector.py ../examples/faces/*.jpg## This face detector is made using the now classic Histogram of Oriented# Gradients (HOG) feature combined with a linear classifier, an image# pyramid, and sliding window detection scheme. This type of object detector# is fairly general and capable of detecting many types of semi-rigid objects# in addition to human faces. Therefore, if you are interested in making# your own object detectors then read the train_object_detector.py example# program. ### COMPILING THE DLIB PYTHON INTERFACE# Dlib comes with a compiled python interface for python 2.7 on MS Windows. If# you are using another python version or operating system then you need to# compile the dlib python interface before you can use this file. To do this,# run compile_dlib_python_module.bat. This should work on any operating# system so long as you have CMake and boost-python installed.# On Ubuntu, this can be done easily by running the command:# sudo apt-get install libboost-python-dev cmake## Also note that this example requires scikit-image which can be installed# via the command:# pip install -U scikit-image# Or downloaded from http://scikit-image.org/download.html. import sysimport dlibfrom skimage import iodetector = dlib.get_frontal_face_detector()win = dlib.image_window()print(&quot;a&quot;);for f in sys.argv[1:]: print(&quot;a&quot;); print(&quot;Processing file: &#123;&#125;&quot;.format(f)) img = io.imread(f) # The 1 in the second argument indicates that we should upsample the image # 1 time. This will make everything bigger and allow us to detect more # faces. dets = detector(img, 1) print(&quot;Number of faces detected: &#123;&#125;&quot;.format(len(dets))) for i, d in enumerate(dets): print(&quot;Detection &#123;&#125;: Left: &#123;&#125; Top: &#123;&#125; Right: &#123;&#125; Bottom: &#123;&#125;&quot; .format(i, d.left(), d.top(), d.right(), d.bottom())) win.clear_overlay() win.set_image(img) win.add_overlay(dets) dlib.hit_enter_to_continue()# Finally, if you really want to you can ask the detector to tell you the score# for each detection. The score is bigger for more confident detections.# Also, the idx tells you which of the face sub-detectors matched. This can be# used to broadly identify faces in different orientations.if (len(sys.argv[1:]) &gt; 0): img = io.imread(sys.argv[1]) dets, scores, idx = detector.run(img, 1) for i, d in enumerate(dets): print(&quot;Detection &#123;&#125;, score: &#123;&#125;, face_type:&#123;&#125;&quot; .format(d, scores[i], idx[i])) 简略总结：1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*-import sysimport dlibfrom skimage import io#使用dlib自带的frontal_face_detector作为我们的特征提取器detector = dlib.get_frontal_face_detector()#使用dlib提供的图片窗口win = dlib.image_window()#sys.argv[]是用来获取命令行参数的，sys.argv[0]表示代码本身文件路径，所以参数从1开始向后依次获取图片路径for f in sys.argv[1:]: #输出目前处理的图片地址 print(&quot;Processing file: &#123;&#125;&quot;.format(f)) #使用skimage的io读取图片 img = io.imread(f) #使用detector进行人脸检测 dets为返回的结果 dets = detector(img, 1) #dets的元素个数即为脸的个数 print(&quot;Number of faces detected: &#123;&#125;&quot;.format(len(dets))) #使用enumerate 函数遍历序列中的元素以及它们的下标 #下标i即为人脸序号 #left：人脸左边距离图片左边界的距离 ；right：人脸右边距离图片左边界的距离 #top：人脸上边距离图片上边界的距离 ；bottom：人脸下边距离图片上边界的距离 for i, d in enumerate(dets):print(&quot;dets&#123;&#125;&quot;.format(d)) print(&quot;Detection &#123;&#125;: Left: &#123;&#125; Top: &#123;&#125; Right: &#123;&#125; Bottom: &#123;&#125;&quot; .format( i, d.left(), d.top(), d.right(), d.bottom())) #也可以获取比较全面的信息，如获取人脸与detector的匹配程度 dets, scores, idx = detector.run(img, 1) for i, d in enumerate(dets): print(&quot;Detection &#123;&#125;, dets&#123;&#125;,score: &#123;&#125;, face_type:&#123;&#125;&quot;.format( i, d, scores[i], idx[i])) #绘制图片(dlib的ui库可以直接绘制dets) win.set_image(img) win.add_overlay(dets) #等待点击 dlib.hit_enter_to_continue() 实例效果： 关键线识别人脸官方代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/usr/bin/python# The contents of this file are in the public domain. See LICENSE_FOR_EXAMPLE_PROGRAMS.txt## This example program shows how to find frontal human faces in an image and# estimate their pose. The pose takes the form of 68 landmarks. These are# points on the face such as the corners of the mouth, along the eyebrows, on# the eyes, and so forth.## This face detector is made using the classic Histogram of Oriented# Gradients (HOG) feature combined with a linear classifier, an image pyramid,# and sliding window detection scheme. The pose estimator was created by# using dlib&apos;s implementation of the paper:# One Millisecond Face Alignment with an Ensemble of Regression Trees by# Vahid Kazemi and Josephine Sullivan, CVPR 2014# and was trained on the iBUG 300-W face landmark dataset.## Also, note that you can train your own models using dlib&apos;s machine learning# tools. See train_shape_predictor.py to see an example.## You can get the shape_predictor_68_face_landmarks.dat file from:# http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2 ## COMPILING THE DLIB PYTHON INTERFACE# Dlib comes with a compiled python interface for python 2.7 on MS Windows. If# you are using another python version or operating system then you need to# compile the dlib python interface before you can use this file. To do this,# run compile_dlib_python_module.bat. This should work on any operating# system so long as you have CMake and boost-python installed.# On Ubuntu, this can be done easily by running the command:# sudo apt-get install libboost-python-dev cmake## Also note that this example requires scikit-image which can be installed# via the command:# pip install -U scikit-image# Or downloaded from http://scikit-image.org/download.html. import sysimport osimport dlibimport globfrom skimage import ioif len(sys.argv) != 3: print(&quot;Give the path to the trained shape predictor model as the first &quot; &quot;argument and then the directory containing the facial images./n&quot; &quot;For example, if you are in the python_examples folder then &quot; &quot;execute this program by running:/n&quot; &quot; ./face_landmark_detection.py shape_predictor_68_face_landmarks.dat ../examples/faces/n&quot; &quot;You can download a trained facial shape predictor from:/n&quot; &quot; http://sourceforge.net/projects/dclib/files/dlib/v18.10/shape_predictor_68_face_landmarks.dat.bz2&quot;) exit()predictor_path = sys.argv[1]faces_folder_path = sys.argv[2]detector = dlib.get_frontal_face_detector()predictor = dlib.shape_predictor(predictor_path)win = dlib.image_window()for f in glob.glob(os.path.join(faces_folder_path, &quot;*.jpg&quot;)): print(&quot;Processing file: &#123;&#125;&quot;.format(f)) img = io.imread(f) win.clear_overlay() win.set_image(img) # Ask the detector to find the bounding boxes of each face. The 1 in the # second argument indicates that we should upsample the image 1 time. This # will make everything bigger and allow us to detect more faces. dets = detector(img, 1) print(&quot;Number of faces detected: &#123;&#125;&quot;.format(len(dets))) for k, d in enumerate(dets): print(&quot;Detection &#123;&#125;: Left: &#123;&#125; Top: &#123;&#125; Right: &#123;&#125; Bottom: &#123;&#125;&quot;.format( k, d.left(), d.top(), d.right(), d.bottom()))# Get the landmarks/parts for the face in box: d.shape = predictor(img, d) print(&quot;Part 0: &#123;&#125;, Part 1: &#123;&#125; ...&quot;.format(shape.part(0), shape.part(1)))# Draw the face landmarks on the screen.win.add_overlay(shape)win.add_overlay(dets)dlib.hit_enter_to_continue() 简化代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# -*- coding: utf-8 -*-import dlibimport numpyfrom skimage import io#源程序是用sys.argv从命令行参数去获取训练模型，精简版我直接把路径写在程序中了predictor_path = &quot;./data/shape_predictor_68_face_landmarks.dat&quot;#源程序是用sys.argv从命令行参数去获取文件夹路径，再处理文件夹里的所有图片#这里我直接把图片路径写在程序里了，每运行一次就只提取一张图片的关键点faces_path = &quot;./data/3.jpg&quot;#与人脸检测相同，使用dlib自带的frontal_face_detector作为人脸检测器detector = dlib.get_frontal_face_detector()#使用官方提供的模型构建特征提取器predictor = dlib.shape_predictor(predictor_path)#使用dlib提供的图片窗口win = dlib.image_window()#使用skimage的io读取图片img = io.imread(faces_path)#绘制图片win.clear_overlay()win.set_image(img) #与人脸检测程序相同,使用detector进行人脸检测 dets为返回的结果dets = detector(img, 1)#dets的元素个数即为脸的个数print(&quot;Number of faces detected: &#123;&#125;&quot;.format(len(dets)))#使用enumerate 函数遍历序列中的元素以及它们的下标#下标k即为人脸序号#left：人脸左边距离图片左边界的距离 ；right：人脸右边距离图片左边界的距离 #top：人脸上边距离图片上边界的距离 ；bottom：人脸下边距离图片上边界的距离for k, d in enumerate(dets): print(&quot;dets&#123;&#125;&quot;.format(d)) print(&quot;Detection &#123;&#125;: Left: &#123;&#125; Top: &#123;&#125; Right: &#123;&#125; Bottom: &#123;&#125;&quot;.format( k, d.left(), d.top(), d.right(), d.bottom())) #使用predictor进行人脸关键点识别 shape为返回的结果 shape = predictor(img, d) #获取第一个和第二个点的坐标（相对于图片而不是框出来的人脸） print(&quot;Part 0: &#123;&#125;, Part 1: &#123;&#125; ...&quot;.format(shape.part(0), shape.part(1))) #绘制特征点 win.add_overlay(shape)#绘制人脸框win.add_overlay(dets)#也可以这样来获取（以一张脸的情况为例）#get_landmarks()函数会将一个图像转化成numpy数组，并返回一个68 x2元素矩阵，输入图像的每个特征点对应每行的一个x，y坐标。def get_landmarks(im): rects = detector(im, 1) return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])#多张脸使用的一个例子def get_landmarks_m(im): dets = detector(im, 1) #脸的个数 print(&quot;Number of faces detected: &#123;&#125;&quot;.format(len(dets))) for i in range(len(dets)):facepoint = np.array([[p.x, p.y] for p in predictor(im, dets[i]).parts()])for i in range(68): #标记点 im[facepoint[i][1]][facepoint[i][0]] = [232,28,8] return im#打印关键点矩阵print(&quot;face_landmark:&quot;)print(get_landmarks(img))#等待点击dlib.hit_enter_to_continue() 效果实例： 参考文档：http://www.th7.cn/Program/Python/201511/706515.shtml","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Python","slug":"Python","permalink":"http://blog.killyliu.com/tags/Python/"}]},{"title":"人脸识别(3)--Python3.6+dlib19.4在Mac下环境搭建","date":"2017-04-20T05:28:38.000Z","path":"2017/04/20/人脸识别-3-Python3-6-dlib19-4在Mac下环境搭建/","text":"前面讲到用OpenCV实现简单的人脸识别，这里用dlib实现关键点检测。 前提条件系统环境：这里是在mac系统下进行开发的，其他系统仅供参考。Python：安装python3.6环境(具体操作见之前的博客)。 dlib之前的准备进入终端安装一系列可能用到的包：安装easy-install：1$ sudo pip install python-setuptools 安装python-dev：1$ sudo pip install python-dev 安装numpy：1$ sudo pip install numpy 安装PIL：1$ sudo pip install Image 安装scipy：1$ sudo apt-get install python-scipy 安装matplotlib：1$ sudo apt-get install python-matplotlib 安装dlib依赖dlib安装需要的依赖有openblas，opencv：12$ brew install openblas$ brew install opencv dlib的so库需要的依赖libboost：1$ sudo pip install libboost-python-dev cmake 安装Mac的X11：&emsp;&emsp;X11是执行Unix程序的图形窗口环境。Mac OS X本身的程序是Aqua界面的，但是为了能够兼容unix和Linux移植过来的程序，需要x11窗口环境。 &emsp;&emsp;运行dlib需要X11，但Mac目前没有自带X11，需要重新下载安装，下载地址为：https://www.xquartz.org/，下载后直接安装，默认安装目录为/opt/X11，需要在/usr/loca/opt目录下创建软连接，创建命令如下，创建后重启Mac。12$ cd /usr/local/opt$ ln -s /opt/X11 X11 安装dlib进入dlib官网下载安装包http://dlib.net/，选择合适位置解压。或者使用git下载：1$ git clone https://github.com/davisking/dlib.git 下载完成后进行解压与安装：12345$ cd dlib/examples$ mkdir build$ cd build$ cmake .. $ cmake --build . --config Release 安装dlib中的python模块：在dlib-18.17及之前的版本中，之后进入python_examples下使用bat文件进行编译，编译需要先安装libboost-python-dev和cmake。12$ cd to dlib-18.17/python_examples$ ./compile_dlib_python_module.bat 在18.18及之后，采用新的方式，用setup.py安装生成so依赖文件：12$ cd dlib$ sudo python setup.py install 在得到dlib.so之后将其复制到dist-packages目录下：1$ sudo cp dlib.so /usr/local/lib/python3.6/dist-packages/ 设置python环境变量：12\\# Put the following line in .bashrc or .profile$ export PYTHONPATH=/path/to/dlib/python_examples:$PYTHONPATH 之后再安装一些可能会用到的依赖包：安装skimage1$ sudo pip install python-skimage 安装imtools1$ sudo pip install imtools 实例检测实例1（会出现X11窗口，打开Mac摄像头自动检测人脸并标注人脸的landmar）：12345$ cd dlib/examples/build/#下载face landmark模型$ wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2# 解压文件$ ./webcam_face_pose_ex 实例2（显示有人脸标记的图片）：1$ ./face_landmark_detection_ex shape_predictor_68_face_landmarks.dat ../faces/2008_002506.jpg 参考文档： http://www.learnopencv.com/facial-landmark-detection/ http://blog.csdn.net/Quincuntial/article/details/53572415 http://www.th7.cn/Program/Python/201511/706515.shtml","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Python","slug":"Python","permalink":"http://blog.killyliu.com/tags/Python/"}]},{"title":"人脸识别(2)--Python3.6+OpenCV3.2识别实例","date":"2017-04-20T05:28:23.000Z","path":"2017/04/20/人脸识别-2-Python3-6-OpenCV3-2识别实例/","text":"前提条件：确保python+opencv环境已经搭建成功(搭建步骤可以参考上一篇博客)。 调用摄像头获取图片进行实时检测调用摄像头，可以看到头像用方框框起来，并实时根据情况调整位置。1234567891011121314151617181920212223242526272829303132#!/usr/bin/env python#coding=utf-8import cv2import numpy as npcv2.namedWindow(&quot;test&quot;)cap=cv2.VideoCapture(0) success, frame = cap.read()color = (0,0,0)classfier=cv2.CascadeClassifier(&quot;/Users/liuqi/opencv/data/haarcascades/haarcascade_frontalface_alt.xml&quot;)while success: success, frame = cap.read() size=frame.shape[:2] image=np.zeros(size,dtype=np.float16) image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) cv2.equalizeHist(image, image) divisor=8 h, w = size minSize =(w//divisor, h//divisor) faceRects = classfier.detectMultiScale(image, 1.2, 2, cv2.CASCADE_SCALE_IMAGE,minSize) if len(faceRects)&gt;0: for faceRect in faceRects: x, y, w, h = faceRect cv2.rectangle(frame, (x, y), (x+w, y+h), color) cv2.imshow(&quot;test&quot;, frame) key=cv2.waitKey(10) c = chr(key &amp; 255) if c in [&apos;q&apos;, &apos;Q&apos;, chr(27)]: breakcv2.destroyWindow(&quot;test&quot;) 识别结果实例： 本地获取图片进行人脸检测并保存图片从本地路径获取图片，识别人脸后将结果图片再保存在本地。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#!/usr/bin/env python#coding=utf-8import osimport sysfrom PIL import Image, ImageDrawimport numpy as npimport cv2#import dlibdef detect_object(infile, save_path): image = cv2.imread(infile) &apos;&apos;&apos;检测图片，获取人脸在图片中的坐标&apos;&apos;&apos; size=image.shape[:2]#获得当前桢彩色图像的大小 #image_set=np.zeros(size,dtype=np.float16)#定义一个与当前桢图像大小相同的的灰度图像矩阵 image_grey = np.zeros(size, np.uint8)#创建一个空白图片 #image_grey = Image.new(mode= &quot;RGBA&quot;,size = size, color = (117,255,0)) #image_grey = cv2.imread(img_grey) grey = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)#将当前桢图像转换成灰度图像 #img_binary = cv2.threshold(image,127,255,0)#将灰度图片转化为二进制图片 color = (135,206,250) #设置人脸框的颜色 #eye_cascade = cv2.CascadeClassifier(&apos;/Users/liuqi/opencv/data/haarcascades/haarcascade_eye.xml&apos;) classfier=cv2.CascadeClassifier(&quot;/Users/liuqi/opencv/data/haarcascades/haarcascade_frontalface_alt.xml&quot;) #人脸检测，1.2和2分别为图片缩放比例和需要检测的有效点数 faceRects = classfier.detectMultiScale(grey, scaleFactor = 1.3, minNeighbors = 4, minSize = (32, 32)) result = [] im = Image.open(infile) if len(faceRects) &gt; 0: #大于0则检测到人脸 draw = ImageDraw.Draw(im) num = 0 for faceRect in faceRects: num += 1 #单独框出每一张人脸 x, y, w, h = faceRect #画出矩形框 #cv2.rectangle(image_grey, (x - 10, y - 10), (x + w + 10, y + h + 10), color, 2) cv2.rectangle(grey, (x - 10, y - 10), (x + w + 10, y + h + 10), color, 2) a = im.crop(faceRect) file_name = os.path.join(save_path,str(num)+&quot;.jpg&quot;) #a.save(file_name) &apos;&apos;&apos;保存新生成的图片&apos;&apos;&apos; #将当前帧保存为图片 #cv2.imwrite(file_name,image_grey) cv2.imwrite(file_name,grey) drow_save_path = os.path.join(save_path,&quot;out.jpg&quot;) im.save(drow_save_path, &quot;JPEG&quot;, quality=80) else: print (&quot;Error: cannot detect faces on %s&quot; % infile) return 0def process(infile): #获取图片，进行检测 #image = cv2.imread(infile) &apos;&apos;&apos;在原图上框出头像并且截取每个头像到单独文件夹&apos;&apos;&apos; #创建输出图片的路径 #im = Image.open(infile) path = os.path.abspath(infile) save_path = os.path.splitext(path)[0]+&quot;_face&quot; try: os.mkdir(save_path) except: pass faces = detect_object(infile, save_path)if __name__ == &quot;__main__&quot;: process(&quot;/Users/liuqi/Desktop/2.jpg&quot;) 实现的效果如下：选取的图片与识别后的图片： &emsp;","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Python","slug":"Python","permalink":"http://blog.killyliu.com/tags/Python/"}]},{"title":" 人脸识别(1)--Python3.6+OpenCV3.2在Mac下环境搭建","date":"2017-04-20T05:28:04.000Z","path":"2017/04/20/人脸识别-1-Python3-6-OpenCV3-2在Mac下环境搭建/","text":"&emsp;&emsp;现在越来越多的地方需要用到智能识别，这里先介绍一种简单的用方框实现人脸检测的环境。 python安装从官网上下载最新版本的python，这里我选择了dmg文件，直接双击进行安装。python官网;https://www.python.org/downloads/mac-osx/ 之后需要更新一下PATH路径:打开cmd，在~/.bash_profile中添加（如果不存在进行添加）12$ vim ~/.bash_profileexport PATH=/usr/local/bin:$PATH 然后重新加载~/.bash_profile，保证更新成功:1$ source ~/.bash_profile 确认python安装成功:1234$ which python3/usr/local/bin/python$ python3 --versionPython 3.6.1 搭建python虚拟环境虽然虚拟环境不是必须的步骤，但是鉴于我们可能用电脑开发很多不同的项目，所以强烈建议新建一个虚拟环境用于python的opencv开发。 首先，安装虚拟环境 virtualenv 和 virtualenvwrapper:1$ pip3 install virtualenv virtualenvwrapper 这个虚拟环境是在python环境中都可以用的。这里我们更新~/.bash_profile 的设置:123#Virtualenv/VirtualenvWrapperexport VIRTUALENVWRAPPER_PYTHON=/usr/local/bin/python3source /usr/local/bin/virtualenvwrapper.sh 然后重新加载.bash_profile:1$ source ~/.bash_profile 现在我们创建一个cv3的虚拟环境进行开发，便于安装一些这个项目需要的额外的包，并进行图片处理。1$ mkvirtualenv cv3 -p python3 在 mkvirtualenv 之后会自动进入cv3环境，但如果是已经存在这个环境，想再次进入环境的话，用worken:1$ workon cv3 可以进入cv3环境。在这个虚拟环境中，我们需要安装numpy（python的先决条件）1$ pip install numpy 安装openVC的先决条件为了编译openVC，需要安装一些开发工具:1$ brew install cmake pkg-config 同时下载一些用于各种图片格式读取的包:1$ brew install jpeg libpng libtiff openexr 以及另外一些包用于优化openVC程序:1$ brew install eigen tbb 编译openVC环境从Github上下载openVC源码（可以从官网上选择最新的版本进行检出）:注： Github地址：https://github.com/opencv/opencv1234$ cd ~$ git clone https://github.com/Itseez/opencv.git$ cd opencv$ git checkout 3.2.0 之后，我们需要下载opencv_contrib包，为OpenCV提供一些额外的支持，像内容检测等(这里我们选择和OpenCV相同的版本进行下载)1234$ cd ~$ git clone https://github.com/Itseez/opencv_contrib$ cd opencv_contrib$ git checkout 3.2.0 在下载完成后，创建build文件夹:123$ cd ~/opencv$ mkdir build$ cd build 使用CMake进行build:注：这里要确认OpenCV和python3模块都加载完成。12345678910$ cmake -D CMAKE_BUILD_TYPE=RELEASE \\ -D CMAKE_INSTALL_PREFIX=/usr/local \\ -D PYTHON3_PACKAGES_PATH=~/.virtualenvs/cv3/lib/python3.4/site-packages \\ -D PYTHON3_LIBRARY=/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/lib/libpython3.4m.dylib \\ -D PYTHON3_INCLUDE_DIR=/usr/local/Cellar/python3/3.4.3/Frameworks/Python.framework/Versions/3.4/include/python3.4m \\ -D INSTALL_C_EXAMPLES=ON \\ -D INSTALL_PYTHON_EXAMPLES=ON \\ -D BUILD_EXAMPLES=ON \\ -D BUILD_opencv_python3=ON \\ -D OPENCV_EXTRA_MODULES_PATH=~/opencv_contrib/modules .. 当CMake完成并且不报错之后，开始进行编译:12$ make -j4$ make install 若权限不足，则使用下面语句:12$ make -j4$ sudo make install 安装验证验证cv2.so正确:123$ cd ~/.virtualenvs/cv3/lib/python3.4/site-packages/$ ls -l cv2.so-rwxr-xr-x 1 admin _developer 2017027 April 14 06:11 cv2.so 验证python中可以使用opencv包，import不报错:1234Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import cv2 参考文档：https://www.pyimagesearch.com/2015/06/29/install-opencv-3-0-and-python-3-4-on-osx/","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Python","slug":"Python","permalink":"http://blog.killyliu.com/tags/Python/"}]},{"title":"Repo的理解及用法小结(2)","date":"2017-04-18T03:07:00.000Z","path":"2017/04/18/Repo的理解及用法小结-2/","text":"&emsp;&emsp;之前已经讲了Repo的基本理解，这里总结一下Repo的常用命令。 1. repo init1repo init -u manifest_git_path -m manifest_file_name -b branch_name --repo-url=repo_url --no-repo-verify &emsp;&emsp;在当前目录下安装 Repo。这会产生一个 .repo/ 目录，目录包括装 Repo 源代码和标准 Android 清单文件的 Git 仓库。.repo/ 目录还包括 manifest.xml，是一个在 .repo/manifests/ 目录选择清单的符号链接。&emsp;&emsp;选项： -u: 指定Manifest库的Git访问路径。-m: 指定要使用的Manifest文件。-b: 指定要使用Manifest仓库中的某个特定分支。–repo-url: 指定要检查repo是否有更新的远端repoGit库的访问路径。–no-repo-verify: 指定不检查repo库是否需要更新。 2. repo sync1repo sync [project_name] &emsp;&emsp;用于参照清单文件克隆并同步版本库。可以使用repo sync project_name的形式只克隆某个项目。。&emsp;&emsp;实现参照清单.repo/manifests.xml克隆并同步版本库，如果版本库不存在，则相当于执行1git clone &emsp;&emsp;如果版本库已经存在，则相当于执行1234#对每个remote源进行fetch操作git remote update#针对当前分支的跟踪分支进行rebase操作git rebase/origin/branch &emsp;&emsp;选项： -d：切换指定项目回到清单修正。如果该项目目前是一个主题分支那就有帮助，但清单修正是暂时需要。-s：同步到一个已知的构建 manifest-server 在当前清单指定的元素。-f：继续同步其他项目，即使有项目同步失败。 3. repo start1repo start &lt;newbranchname&gt; [--all|&lt;project&gt;...] &emsp;&emsp;创建并切换分支。刚克隆下来的代码是没有分支的，repo start实际是对git checkout -b命令的封装。&emsp;&emsp;为指定的项目或所有的项目（若使用-all），以清单文件中为设定的分支，创建特定的分支。&emsp;&emsp;这条指令与git checkout -b 还是有很大区别的。&emsp;&emsp;· git checkout -b 是在当前所在的分支的基础上创建特性分支。&emsp;&emsp;· 而repo start 是在清单文件设定的分支的基础上创建特性分支。1repo start stable --all &emsp;&emsp;假设清单文件中设定的分支是gingerbread-exdroid-stable，那么执行以上指令就是对所有项目，在gingerbread-exdroid-stable的基础上创建特性分支stable。1repo start stable platform/build platform/bionic &emsp;&emsp;假设清单文件中设定的分支是gingerbread-exdroid-stable，那么执行以上指令就是对platform/build、platform/bionic项目，在gingerbread-exdroid-stable的基础上创建特性分支stable。 4. repo checkout123&lt;branchname&gt; [&lt;rpoject&gt;...]&#123;&#123;&#123; repo checkout &lt;branchname&gt; [&lt;project&gt;...]&#125;&#125;&#125; &emsp;&emsp;切换分支。 实际上是对git checkout命令的封装，但不能带-b参数，所以不能用此命令来创建特性分支。&emsp;&emsp;示例：12repo checkout liuq-devrepo checkout liuq-dev skipper/build platform/bionic 5. repo branches1repo branches [&lt;project&gt;...] &emsp;&emsp;查看分支。&emsp;&emsp;示例：12345repo branchesrepo branches skipper/build skipper/release#查看可切换的分支cd .repo/manifestsgit branch -a | cut -d / -f 3 6. repo diff1repo diff [&lt;project&gt;...] &emsp;&emsp;查看工作区文件差异。实际是对git diff命令的封装，用于分别显示各个项目工作区下的文件差异。在 commit 和工作目录之间使用 git diff 显示明显差异的更改。&emsp;&emsp;示例：1234#查看所有项目repo diff#只查看其中的两个项目repo diff skipper/build skipper/release 7. repo stage1repo stage -i [&lt;project&gt;...] &emsp;&emsp;把文件添加到index表中。实际上是对git add –interactive命令的封装，用于挑选各个项目中的改动以加入暂存区。&emsp;&emsp;-i表示git add –interactive命令中的–interactive，给出一个界面供用户选择。 8. repo prune1repo prune [&lt;project&gt;...] &emsp;&emsp;删除已经合并分支。实际上是对git branch -d 命令的封装，该命令用于扫描项目的各个分支，并删除已经合并的分支。 9. repo abandon1repo abandon &lt;branchname&gt; [&lt;rpoject&gt;...] &emsp;&emsp;删除指定分支。实际是对git brance -D命令的封装。 10. repo status1repo status [&lt;project&gt;...] &emsp;&emsp;查看文件状态。&emsp;&emsp;示例：12#输出skipper/build项目分支的修改状态repo status skipper/build 每个小节的首行显示项目名称，以及所在的分支的名称。每个字母表示暂存区的文件修改状态。 字母 含义 描述 - 无变化 没有修改，在 HEAD 和在索引中是一样的 A 添加 不在HEAD中，在暂存区中 M 修改 在HEAD中， 在暂存区中，内容不同 D 删除 在HEAD中，不在暂存区 R 重命名 不在HEAD中，在暂存区中 C 拷贝 不在HEAD中，在暂存区，从其他文件拷贝 T 文件状态改变 在HEAD中，在暂存区，内容相同 U 未合并 需要冲突解决 第二个字符表示工作区文件的更改状态。 字母 含义 描述 - 新/未知 不在暂存区，在工作区 m 修改 在暂存区，在工作区，被修改 d 删除 在暂存区，不在工作区 两个表示状态的字母后面，显示文件名信息。如果有文件重名还会显示改变前后的文件名及文件的相似度。 11. repo remote12repo remote add &lt;remotename&gt; &lt;url&gt; [&lt;project&gt;...]repo remote rm &lt;remotename&gt; [&lt;project&gt;...] &emsp;&emsp;设置远程仓库。&emsp;&emsp;示例：1repo remote add org ssh://10.11.10.11/git_repo &emsp;&emsp;这个指令根据xml文件添加的远程分支，方便于向服务器提交代码，执行之后的build目录下看到新的远程分支org。12#删除远程仓库repo remote rm org 12. repo push1repo push &lt;remotename&gt; [--all|&lt;project&gt;...] &emsp;&emsp;向服务器提交代码。repo会自己查询需要向服务器提交的项目并提示用户。&emsp;&emsp;示例：1repo push org 13. repo forall1repo forall [&lt;project&gt;...] -c &lt;command&gt; &emsp;&emsp;迭代器，可以在所有指定的项目中执行同一个shell指令。&emsp;&emsp;选项： -c 后面所带的参数是shell指令，即执行命令和参数。命令是通过 /bin/sh 评估的并且后面的任何参数就如 shell 位置的参数通过。-p 在shell指令输出之前列出项目名称，即在指定命令的输出前显示项目标题。这是通过绑定管道到命令的stdin，stdout，和 sterr 流，并且用管道输送所有输出量到一个连续的流，显示在一个单一的页面调度会话中。-v 列出执行shell指令输出的错误信息，即显示命令写到 sterr 的信息。 &emsp;&emsp;附加环境变量： REPO_PROJECT 指定项目的名称REPO_PATH 指定项目在工作区的相对路径REPO_REMOTE 指定项目远程仓库的名称REPO_LREV 指定项目最后一次提交服务器仓库对应的哈希值REPO_RREV 指定项目在克隆时的指定分支，manifest里的revision属性 &emsp;&emsp;如果-c后面所带的shell指令中有上述环境变量，则需要用单引号把shell指令括起来。 13.1. 添加环境变量12repo forall -c &apos;echo $REPO_PROJECT&apos;repo forall -c &apos;echo $REPO_PATH&apos; 13.2. 合并多个分支1repo forall -p -c git merge topic &emsp;&emsp;把所有项目都切换到master分支，执行上述指令将topic分支合并到master分支。 13.3. 打标签1repo forall -c git tag crane-stable-1.6 &emsp;&emsp;在所有项目下打标签。 13.4. 设置远程仓库1repo forall -c &apos;git remote add korg ssh://xiong@172.16.31/$REPO_PROJECT.git&apos; &emsp;&emsp;引用环境变量REPO_PROJECT添加远程仓库。12#删除远程仓库。repo forall -c git remote rm korg 13.5. 创建特性分支12repo forall -c git branch crane-devrepo forall -c git checkout -b crane-dev 14. repo grep1repo grep &#123;pattern | -e pattern&#125; [&lt;project&gt;...] &emsp;&emsp;打印出符合某个模式的行。相当于对 git grep 的封装，用于在项目文件中进行内容查找。&emsp;&emsp;示例：1234#要找一行, 里面有#define, 并且有&apos;MAX_PATH&apos; 或者 &apos;PATH_MAX&apos;:repo grep -e &apos;#define&apos; --and -\\( -e MAX_PATH -e PATH_MAX \\)#查找一行, 里面有 &apos;NODE&apos;或&apos;Unexpected&apos;, 并且在一个文件中这两个都有的.repo grep --all-match -e NODE -e Unexpected 15. repo manifest1repo manifest [-o &#123;-|NAME.xml&#125; [-r]] &emsp;&emsp;manifest检验工具，用于显示manifest文件内容。&emsp;&emsp;选项: -h, –help 显示这个帮助信息后退出-r, –revision-as-HEAD 把某版次存为当前的HEAD-o -|NAME.xml, –output-file=-|NAME.xml 把manifest存为NAME.xml 16. repo version1repo version &emsp;&emsp;显示repo的版本号。&emsp;&emsp;选项: -h, –help 显示这个帮助信息后退出. 17. repo upload1repo upload [--re --cc] &#123;[&lt;project&gt;]...|--replace &lt;project&gt;&#125; &emsp;&emsp;repo upload 相当于git push，但是又有很大的不同。它不是将版本库改动推送到代码审核服务器（Gerrit软件架设）的特殊引用上，使用SSH协议。代码审核服务器会对推送的提交进行特殊处理，将新的提交显示为一个待审核的修改集，并进入代码审核流程，只有当审核通过后，才会合并到官方正式的版本库中。&emsp;&emsp;选项： -h, –help 显示帮助信息-t 发送本地分支名称到Gerrit代码审核服务器–replace 发送此分支的更新补丁集–re=REVIEWERS 要求指定的人员进行审核–cc=CC 同时发送通知到如下邮件地址 18. repo download1repo download &#123;project change[/patchset]&#125;... &emsp;&emsp;repo download命令主要用于代码审核者下载和评估贡献者提交的修订。&emsp;&emsp;贡献者的修订在Git版本库中refs/changes//引用方式命名（缺省的patchset为1），和其他Git引用一样，用git fetch获取，该引用所指向的最新的提交就是贡献者待审核的修订。&emsp;&emsp;使用repo download命令实际上就是用git fetch获取到对应项目的refs/changes//patchset&gt;引用，并自动切换到对应的引用上。 19. repo selfupdate1repo selfupdate &emsp;&emsp;用于 repo 自身的更新。如果有新版本的repo存在, 这个命令会升级repo到最新版本。通常这个动作在repo sync时会自动去做, 所以不需要最终用户手动去执行。&emsp;&emsp;选项: -h, –help 显示这个帮助信息后退出.–no-repo-verify 不要验证repo源码. 20. repo help1repo help [--all|command] &emsp;&emsp;显示命令的详细帮助。&emsp;&emsp;选项: -h, –help 显示这个帮助信息后退出-a, –all 显示完整的命令列表 参考文档：http://blog.sina.com.cn/s/blog_89f592f50100vpau.htmlhttps://github.com/Trawn/repo-help-zh-cn/blob/master/repo%20help%20zhcn.txt","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Repo","slug":"Repo","permalink":"http://blog.killyliu.com/tags/Repo/"}]},{"title":"Repo的理解及用法小结(1)","date":"2017-04-17T01:45:09.000Z","path":"2017/04/17/Repo的理解及用法小结-1/","text":"Repo介绍&emsp;&emsp;随之移动终端设备的普及，各企业越来越重视Android市场，而研究Android系统的底层驱动开发，首先需要做的便是建立一套Android版本库管理机制。Android使用Git进行代码管理，而Repo命令行工具对Git命令进行了封装，可以管理多个git项目，从而更好地对代码进行集中式管理。 &emsp;&emsp;Repo是Google用Python脚本写的调用Git的脚本（可以在Google Group 上找到相关文档资料），用于下载、管理Android项目的软件仓库。Repo主要包含Repo配置信息以及Repo锁管理的Git项目集合。 Repo用法安装Repo注：下载Repo只针对第一次使用。首先，确认主目录下存在 bin/ 目录并已包含在路径中：12mkdir ~/binPATH=~/bin:$PATH 下载Repo工具并设置可执行的权限：12curl https://storage.googleapis.com/git-repo-downloads/repo &gt; ~/bin/repochmod a+x ~/bin/repo 注：在Repo设置成功后，会出现下列提示，说明可以进行初始化了。 error: repo is not installed. Use “repo init” to install it here. Repo初始化 (init)首先创建目录用于存放工程文件：12mkdir WORKSPACEcd WORKSPACE 之后，使用命令进行初始化：1repo init -u URL [OPTIONS] 具体操作有： -u：指定一个URL，其连接到一个manifest仓库。-m：在manifest仓库中选择一个 NAME.xml 文件。-b：选择一个manifest仓库中一个特殊的分支。注：· 如果不提供 -b REVISION 或者 –manifest-branch=REVISION参数，则检出 manifest Git 库的 master 分支。· 如果不提供 -m NAME.xml 或者 –manifest-name=NAME.xml 参数，则使用缺省值 default.xml。 在初始化完成后，输入ls命令：1ls repo 可以看到相关文件：manifests/ manifests.git/ manifest.xml repo/ (1) .repo：Repo目录，可用于提取相应项目工作目录到外面的repo工作目录。(2) .repo/manifests.git：Repo配置信息的Git库，不同版本包含不同配置信息。(3) .repo/manifests：Repo配置信息的工作目录（将配置信息的工作目录和相应的实际Git目录分离管理，并且配置信息中的.git目录实际只是指向实际Git库的软连接），其中可能包含一个或多个xml文件描述的配置。每个xml文件是独立的一套配置，配置内容包括当前Repo工作目录包含哪些Git项目、所有Git项目所处的默认公共分支、以及远端地址等。(4) .repo/manifest.xml：Repo工作目录中的内容同一时刻只能采用manifests中的一个xml文件做为其配置，该文件就是其软连接，通过init的-m选项指定采用哪个文件；另外，同一xml文件也可能处于manifests库的不同版本或者不同分支，通过init的-b选项指定使用manifests中的哪个分支，每次init命令都会从服务器更新最新的配置。(5) .repo/repo：Repo脚本集的Git库，这里包含Repo命令所需的所有子命令脚本实现，由Python完成，这个目录本身又由Git来管理。 Repo同步 (sync)下载当前repo配置的所有项目，并生成对应的repo工作目录：1repo sync 如果想生成特定项目的代码，则使用下列语句：1repo sync [&lt;project&gt;...] 实现参照清单.repo/manifests.xml克隆并同步版本库，如果版本库不存在，则相当于执行1git clone 如果版本库已经存在，则相当于执行1234#对每个remote源进行fetch操作git remote update#针对当前分支的跟踪分支进行rebase操作git rebase/origin/branch 同步涉及到的参数有： -j：开启多线程同步操作，会加快sync命令的执行速度。默认情况下，使用4个线程并发进行sync。-c, –current-branch：只同步指定的远程分支。默认情况下，sync会同步所有的远程分支，当远程分支比较多的时候，下载的代码量就大。使用该参数，可以缩减下载时间，节省本地磁盘空间。-d, –detach：脱离当前的本地分支，切换到manifest.xml中设定的分支。在实际操作中这个参数很有用，当我们第一次sync完代码后，往往会切换到dev分支进行开发。如果不带该参数使用sync， 则会触发本地的dev分支与manifest设定的远程分支进行合并，这会很可能会导致sync失败。-f, –force-broken：当有git库sync失败了，不中断整个同步操作，继续同步其他的git库。–no-clone-bundle：在向服务器发起请求时，为了做到尽快的响应速度，会用到内容分发网络(CDN, Content Delivery Network)。同步操作也会通过CDN与就近的服务器建立连接， 使用HTTP/HTTPS的$URL/clone.bundle来初始化本地的git库，clone.bundle实际上是远程git库的镜像，通过HTTP直接下载，这会更好的利用网络带宽，加快下载速度。 同步完成后，.repo下多了projects目录，，原工作目录下也多了目录： (1) .repo/projects：Repo所管理的所有Git项目集，包含Repo当前配置所指定的所有Git项目对应的Git目录。(2) .repo/../ :Repo的工作区。在Repo目录（即.repo）之外，根据Repo配置（即.repo/manifest.xml文件），从.repo/projects下提取出指定分支的各个Git项目（即.repo/projects中Git项目的子集）的工作目录，形成Repo工作目录，可供开发使用。 注：以上内容参考链接http://blog.chinaunix.net/uid-9525959-id-4534319.htmlhttp://www.360doc.com/content/14/0220/17/97538_354256755.shtml","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Repo","slug":"Repo","permalink":"http://blog.killyliu.com/tags/Repo/"}]},{"title":"《黑洞》背后的思考","date":"2017-04-10T02:52:16.000Z","path":"2017/04/10/《黑洞》背后的思考/","text":"&emsp;&emsp;其实最初是想写关于《人民的名义》观看感想的，但思来想去，现在只播出不到一半而已，而且对于最高人民检察院影视中心的组织创作，个人还是寄予了很高的期望。&emsp;&emsp;因而，决定先回顾一部很多年前的反腐老剧《黑洞》，真实、黑暗，又不颓废、不做作。 人性是物性的绽放&emsp;&emsp;”当我们相信自己对这个世界已经相当重要的时候，其实这个世界才刚刚准备原谅我们的幼稚。”再次追剧后，对《少年凯哥》中的这句话有了更深刻的理解。我们总以为自己看懂了一些事，明白了一些物，就可以笑傲江湖。但其实百年岁月仅三千瓢中一引，殊不知天很高，地很厚，水很深。 &emsp;&emsp;刘振汉：这个世界上总有那么一群人，有自己的原则，这个原则不包含任何东西，甚至自己最亲近的人，只为心中的那份执着，那份正义。这样子的人在外人看来，有些异类。但从大局上讲，这样的人是值得尊敬的。 &emsp;&emsp;聂明宇：并非一个纯粹的坏人，只是选择了在处理事情上顺从内心阴暗面的处理方法，以至于走向犯罪的深渊，触犯了法律的高压线。但其实他的一系列行为借用那句“我本是活腻了的人”，只是用不恰当的行为对周边实施了报复，以填补内心的缺失。 &emsp;&emsp;两位各有自己的性格，也各有自己坚守，同样也各有各的得失。这种人性的矛盾成为整个剧的粘合剂。性有如一团颜色各异缠丝，包含着众多的情感，包含着众多的人情事故，也包含着众多的道德准则和自我立场。而不同的抽丝选择，决定了不同的余留种类。 &emsp;&emsp;现实不等于真实、真实不等于真相、真相不等于真理、真理不等于智慧，智慧不等于平凡，最后落在平凡，又和我们的平凡不太一样，是让所有的生命完成和解，和解不是妥协，不是谅解。 &emsp;&emsp;现实社会是在正义、道德等掩饰性词语的背后正恶平衡推动的。对与错没有唯一的标准，智慧、力量、勇气、美好永远都是人们渴求的，而善良，一直是人类发展中的奢侈品。在当时的社会，不谈什么正义、公平、法制，能有自己精神的内核就已经超越了社会的大多数。 经济发展与社会生活的黑洞&emsp;&emsp;《黑洞》中所提到的天都市属于杜撰，这个城市究竟是哪里并不重要，真正重要的是埋在这个城市的土里的时代印记。世纪之交，很多城市看起来繁荣昌盛，发展稳定，但是地下却暗流涌动。时代的发展就像是巨大的车轮无情地碾过，人在很多时候在欲望和权利面前被推着向前走，正如那几乎一手遮天的天都市。 &emsp;&emsp;社会道德和社会良知的黑洞笼罩着天都市，正义与邪恶看似一线之隔，但从黑洞一角抽丝剥茧，你会发现没有尽头，很多事情远比想象中复杂，比预计更艰难。刘振汉的纠结与果敢，聂明宇的从容与不安，龚倩的性情与执念，张峰的绝情与无奈，王明的急性与奉献，小芮的狠毒与忠心，贺清明的胆怯与良善，人性的两面，在此彰显。 &emsp;&emsp;当片尾的音乐前奏响起，冗长而低沉的男声渐吟，那一幕幕黑白交替的画面就这样映射在脑海中，被悲伤与黑暗笼罩着的这个名叫天都的城市，如同深渊一般，将故事里的每一个人都搅进了这个黑洞的漩涡之中。然而，它最终还是无情地制造出了一场悲剧。 &emsp;&emsp;“这些高墙还真是有点意思。一开始你恨它，然后你对它就习惯了。等相当的时间过去后，你还会依赖它。”大势所向，螳臂当车。可大势之外有大势，小小的螳螂怎么看到的天？","tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.killyliu.com/tags/随笔/"}]},{"title":"她是七月，也是安生","date":"2017-04-05T09:43:18.000Z","path":"2017/04/05/她是七月，也是安生/","text":"&emsp;&emsp;很早之前就读过安妮宝贝的这篇小说，些许矛盾，又太过苍白。前两天偶然间看到了电影片段，周冬雨那强忍着眼泪的镜头令人感到莫名心酸，决定认真看一次影视版的《七月与安生》。不得不说，改编后的电影真心很棒，并未落入俗套，而且情节也更加合理。 渴望极致的自由，也想要无悔的安定&emsp;&emsp;从小如影相随的七月和安生，一个装疯一个卖乖，一起在生活中叛逆，又一起憧憬着未来。她们朝夕陪伴互相依赖，是彼此的快乐和慰藉。 &emsp;&emsp;正如电影的英文版标题soulmate，看起来完全不同的两人走在一起并非偶然，其实对方都是自己心里所渴望的那个截然不同的自己，或安静，或放肆，或叛逆，或崇高，或虚伪。七月的温婉安静，是因为她在压抑自我；安生的洒脱不羁，更像是为了掩盖内心对于爱的渴望。 &emsp;&emsp;七月那么安静可爱学习优秀前途大好充满了安全感，安生那么活泼灵动叛逆不羁充满了不确定感。而苏家明是一个青春里的符号，学习优秀、长相端正、品行优良，挺拔，有温暖的笑容，是男神，也是邻家男孩。 &emsp;&emsp;优秀内敛的七月爱上家明，再自然不过。&emsp;&emsp;英俊温和的家明接受七月，再自然不过。 &emsp;&emsp;淳朴端正的家明遇上安生，没有躲过，再正常不过。&emsp;&emsp;叛逆不羁的安生遇上家明，也动心了，再正常不过。 &emsp;&emsp;谁不需要安全感中的稳定，而谁又不需要不确定感中的诱惑。苏家明与七月是日久生情，爱七月是顺其自然；而对安生是一见钟情，爱安生是命运使然。自始至终，苏家明都在摇摆。徘徊在两个女孩儿之间，徘徊在现实与梦之间，多情却理智，聪明却迷茫。他就像一条自由与温暖的分界线，横亘在七月与安生之间，成为她们的芥蒂，却也将彼此紧紧联结。 此岸与彼岸，一个被选择的结果 &emsp;&emsp;其实两人最初的选择都只是合适而已。七月问“你真的爱他吗？”。“他天天唱歌给我听，跟我一起喝酒，一起睡觉，就差没有跟我们俩一样一起洗澡了。”安生没有正面回答，就像当初安生问七月是否真的很爱家明，七月也没有正面回答，只是说“他长得帅，还是田径队的。” &emsp;&emsp;安生为了七月选择了离开，越走越远。去不同的城市，见不同的人，过不同但始终是漂泊的生活。七月则按部就班的生活，安稳的学习，安稳的工作。安生四海为家,走过千山万水，哭过、笑过、闹过、悲伤过、无奈过。在七月看来，这样丰富多彩的生活，其实她是挺羡慕的，比自己“一眼就可以看到一生”的安稳要精彩很多。 &emsp;&emsp;性格相似的适合为友，互补的适合做恋人。但其实反过来也一样。一段稳固的感情，无非就是求同存异。完全相像或者完全相反的人，不会擦出火花。七月是安生的渴望,渴望像她一样的安稳。安生是七月的向往，向往她自由自在的生活。 &emsp;&emsp;“你是什么人，可能家明不知道，那我还不清楚吗？”的确，七月与安生彼此都太了解彼此，而两个人在彼此面前又都是自卑的，在摩擦中互相折磨，在羡慕中互相妒恨。好像自己被一些人吸引着，同时对他们向住着，但心里却妒忌着。 我活成了你，我也只有你&emsp;&emsp;安生对七月像是要抓住浮萍往岸上爬，她不想要那么多狗血、那么多诡谲，只想静静地把心熨平整。经历了生活苦难，尝尽人间辛酸的女孩儿是懂事的，体贴的，犹如她用“不，每个人都不容易的”来回答七月那句任性的“你以为我这些年就容易吗？” &emsp;&emsp;七月展现着自己与安生的不同，解释着苏家明喜欢的是细水长流的小幸福而不是变幻莫测的新鲜刺激。情不知所起，一往而深；而恨见缝插针，无处躲藏。 &emsp;&emsp;她们是彼此的另一面。她回来了，她走了。两个最好的朋友，彼此珍惜着，又彼此羡慕着，两个人通过对方去感受着不同的生活。最后的最后，七月变成了安生，安生也成了七月。就像人生的戏，改变了一下出场的顺序，却是不一样的结局。 &emsp;&emsp;她们是不幸的，当年长后，当彼此终于成熟到能意识到家明不过就是她们友谊中的一次大事故，而她们不值的为事故而放弃她们爱的生命时，七月却永远离去。只留下安生永远留下咀嚼这青春往事。但她们也幸运的。随着七月的离去，友谊长存了，她们彻底融解了，她们永远活在了27岁前，永远是青春里为爱可以放逐自己，为爱可以隐忍自己，彼此都是彼此心头一袭明月的那两个女孩。永远年轻。 &emsp;&emsp;曾经那些炙热的、拼了命想要的，都不重要了。绕了一大圈、一大圈，安生终于心安了。七月也是，电影的最后七月一个人走在遍地雪白的道路上，向前走，好像也已经找到了自己的归处。 &emsp;&emsp;“过得折腾一点，也不一定不幸福，就是太辛苦了。但其实，女孩子不管走哪条路，都是会辛苦的。”&emsp;&emsp;“我知道。”&emsp;&emsp;“希望我的女儿能是个例外。” &emsp;&emsp;我想七月母亲的话适用于任何人，我们既渴望漂泊又渴望安稳，我们既想要辛苦的折腾，又想要成为一个例外。“七月心里很清楚，从那一刻起，自己和安生注定要过上截然不同的人生。”终有一段路是一边哭一边走完。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.killyliu.com/tags/随笔/"}]},{"title":"博客搭建(2)--Mac下用Hexo搭建个人博客","date":"2017-04-04T07:54:30.000Z","path":"2017/04/04/博客搭建-2-Mac下用Hexo搭建个人博客/","text":"在此之前，有人推荐使用jekyll，有人推荐hexo，因而上网查找资料进行简单对比： Jekyll基于Ruby， Hexo基于NodeJs。 两者都支持Markdown。 Jekyll的主题相对而言没有Hexo的美观（个人见解） 对于网上所说的Jekyll没有本地服务器，需要纠正一下，其实两者都可以实现本地预览功能，再发布到容器中同步。根据个人喜好进行选择，这里我选择了hexo。 准备工作1. 安装node.js 安装语句：brew install node 2. 已搭建git环境（或者github管理软件） 使用语句查看安装成功：node -vnpm -vgit –version 初始化Github环境在上一篇博客中已经建好了网站，这里直接克隆到本地便于接下来的操作。选定博客搭建路径后，将仓库克隆到本地： git clone https://github.com/Killy-LIU/killy-liu.github.io.gitcd killy-liu.github.io/git init 安装Hexo并搭建网站 1. 安装执行命令： sudo npm install -g hexo-cli 2. 从控制台进入克隆的文件夹目录下，进行初始化hexo： hexo init 3. 在初始化完成之后，执行本地服务化发现报错。因而重新进行安装配置。 npm install 文件夹目录结构 source：博客资源文件夹source/_drafts：草稿文件夹source/_posts：文章文件夹themes：存放主题的文件夹themes/landscape：默认的主题_config.yml：全局配置文件 4. 进行本地服务测试。 hexo server 输入 http://localhost:4000/ 即可打开默认页面： 跳坑： 这里曾经因为无法启动本地服务器，尝试过许多方法(如下)，均无法解决错误 Cannot find module ‘./build/Release/DTraceProviderBindings’，以下是曾经尝试过的方式： npm install hexo –no-optionalnpm uninstall hexo + npm install hexo –no-optionalnpm uninstall hexo-cli -g + npm install hexo-cli -g 最后查找资料得到的解决方案： 首先我们找到全局hexo的安装目录 找到文件dtrace-provider.js 注释如下内容:12345678910111213141516 var builds = [&apos;Release&apos;, &apos;default&apos;, &apos;Debug&apos;];for (var i in builds) &#123; try &#123; var binding = require(&apos;./build/&apos; + builds[i] + &apos;/DTraceProviderBindings&apos;); DTraceProvider = binding.DTraceProvider; break; &#125; catch (e) &#123; // if the platform looks like it _should_ have DTrace // available, log a failure to load the bindings. if (process.platform == &apos;darwin&apos; || process.platform == &apos;sunos&apos; || process.platform == &apos;freebsd&apos;) &#123; console.error(e); &#125; &#125;&#125; 更换主题现在网站上的主题是默认的，既然是打造属于自己的小窝，必然要考虑换一个更好看的样式了~~在经过仔细比对之后，选择了可以适配网页端和手机端的特色主题yilia，相信自己的审美哦^_^列出排名前三的主题： 1.https://github.com/iissnan/hexo-theme-next 3510个star。2.https://github.com/litten/hexo-theme-yilia 1703个star。3.https://github.com/TryGhost/Casper 679个star 具体操作步骤： 1. 克隆项目到本地(任意一个不是在博客文件夹里的地址均可)：这里我用github克隆的，与控制台的命令语句效果相同： git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 克隆结束后，将克隆文件夹名更改为 yilia，并放于之前所建目录下的/themes/下，与landscape并列。 2. 修改./_config.yml文件： theme: yilia //默认为landscape 3. 修改themes/yilia/_config.yml文件： 注：根据自己的实际情况做调整：动画效果、友情链接、博友回复等等。 4. 查看效果更改主题之后可以使用命令 hexo server 打开本地服务，再次登录 http://localhost:4000/ 查看效果。 Github部署这里连着上一篇博客，将搭建好的网站通过修改配置，完成在github上的部署。 1. Github设置修改./_config.yml文件： deploy: type: git repository: https://github.com/Killy-LIU/killy-liu.github.io.git branch: master 注：在hexo3.x版本下，type应填git，而非github；冒号后面都有一个英文的空格，不然会报错。 2. 部署部署命令为： hexo clean (清理public文件夹，不需要经常用)hexo generatehexo deploy 但我的上一条命令 hexo deploy 报错，因而采用下面的命令： npm install hexo-deployer-git–savehexo deploy 跳坑1：理论上应该部署成功，可是点击网页出现404，发现网页没有对应到我设置的网站上。 百度一下，需要将CNAME，LICENSE，README.md 放在source文件夹下，部署时才会一起传到github上。 重新运行 hexo generatehexo deploy 部署成功^_^ 跳坑2：好多博客中写先搭建hexo并选择主题，后部署与github。曾经试过用github将上一期得到的内容直接下载下来，将这里的内容拷贝到文件夹内，但启动时报错：[rejected] master -&gt; master (non-fast-forward)。原因是两个都是master，有冲突。","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Hexo","slug":"Hexo","permalink":"http://blog.killyliu.com/tags/Hexo/"}]},{"title":"博客搭建(1)--使用Github搭建个人博客","date":"2017-04-04T05:59:17.000Z","path":"2017/04/04/博客搭建-1-使用Github搭建个人博客/","text":"&ensp;ps：因为最近手误删了两次csdn博客，因而决定在个人博客上也备份一份/(ㄒoㄒ)/~~ 基本准备 github账号：可以起一个有意义的名字哦&emsp;&ensp;链接地址： https://github.com 域名购买：可以在腾讯云或者阿里云上购买一个域名（都有针对学生的优惠）&emsp;&ensp;腾讯云地址：https://www.qcloud.com/?fromSource=gwzcw.5677.5677.5677&emsp;&ensp;阿里云链接地址：https://www.aliyun.com/?utm_medium=text&amp;utm_source=bdbrand&amp;utm_campaign=bdbrand&amp;utm_content=se_32492 创建页面仓库链接地址：https://github.com/new填写相应的信息：仓库名，描述，public，可选初始化信息。 创建成功之后点击设置，可以看到具体的信息： 下拉并点击主题选择，可以根据自己的喜好设置*（注：这里选择的模板都是jekyll的，之后会调整到hexo）： 完成后点击设置可以看到网站链接，便可以访问自己的网站啦啦啦~ 绑定域名跳坑：之前已经拥有了域名：https://killy-liu.github.io/killyliu.github.io/，发现重复了信息，因此修改自己的repo名与自己的名字一样，这里都改为killy-liu.github.io,地址即变为https://killy-liu.github.io/，指向刚刚构建好的页面。 现在需要在根目录下创建一个CNAME的文件，里面写自己的地址，www.killyliu.com，之后访问内容即会重定向到自己这个地址。与此同时，需要进入云服务中设置域名解析。这里我用的是腾讯云服务。在记录管理中对添加域名记录，如下图所示。 在添加完成之后用控制台输入ping命令，查看是否通顺。 试着访问http://www.killyliu.com/，即可看到自己的第一个博客页面^_^ 下一步操作为设置博客样式以及完善内容，并且在思考之后将个人链接由 http://www.killyliu.com 转为 http://blog.killyliu.com 很抱歉之前不能点击链接~~","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"Github","slug":"Github","permalink":"http://blog.killyliu.com/tags/Github/"}]},{"title":"过善于思考的人生，别总选易走的路","date":"2017-03-30T12:31:20.000Z","path":"2017/03/30/过善于思考的人生，别总选易走的路/","text":"&emsp;&emsp;昨天去闺蜜家玩，两人不知不觉聊到了生活方向这个问题，感触颇深，决定写篇随笔记录一下~ &emsp;&emsp;洋学习影视表演专业，现在自己开起了服装网店，日子看起来即使不是特别潇洒也还算自由自在。其实周围好多人都有疑惑，看似不搭调的两个人是如何成为闺蜜的。这里顺便解释一下，我俩是小学同学，到现在已经有十余载的年岁了，虽然走的路不同，但一种似亲情的友谊是与学识经历这些关系不大的。 &emsp;&emsp;回想之前她在化大当老师的时候，就总在业余时间做兼职，两人见面基本都会谈起将来道路选择问题。没有人是真正轻松的，我的付出在于这些年的努力学习，而她的辛苦则体现在各种人际社会关系的处理。轻松欢乐的表面背后，都是不为人知的付出。 &emsp;&emsp;这次，聊到了将来的工作选择上，洋又一次点醒了我。大四时候曾想过自己创业，但在了解一系列流程操作之后发现前路坎坷迷茫而毅然选择了放弃，只想着安安稳稳找家国企有个户口就好。不求轰轰烈烈，只求平平淡淡。现在并不是说这个想法不好，只是缺少了一种趁着年轻勇于奋斗的拼劲。如果二十多岁就过着养老生活，岂不白白浪费青春年华？ &emsp;&emsp;在北京两个非常现实的问题：一是户口，二是住房。现在想起大三时的自己还真的是任性，借着有出国留学经历就觉得即使不读研也可以在企业中风生水起。当时固执地坚持着保不了研就工作的理念，庆幸自己的成绩帮自己选了一条不至于后悔的路，离户口和住房更近了一步。而前一阵子炒饭传给我一张自己写的大二规划的照片，不禁感慨，原来大二自己的思想已经如此成熟了，而且再次体会到思考与规划的价值所在。 &emsp;&emsp;两周前和梦莹聊到她的规划，在企业里做金融方面的计算机实习或工作，空闲时间再自己搭个网站，逐步运营起来~相较而言梦莹的生活更加潇洒，寻找自己喜欢的城市呆十年八年，不用被太多的身外之物所牵绊。奈何自己做不到这么洒脱，旅游是一种放空心灵的方式，但它于我只能是一种附属而非生活。 &emsp;&emsp;今天项目组开会时候调侃道我们不断思考并发task归根到底还是因为穷，如果真能有几百TB级的服务器就都不是事了。穷则思变，而善思则掌局。既然选择了在北京发展，又并非含着金汤匙出生的公主，就应该认真对待生活，仔细规划发展道路。即使没有太大的抱负要感天动地，也不能抱着得过且过的思想，毕竟能挥霍的日子并不多了。 &emsp;&emsp;回到之前的现实状况：户口方面存在两个选择：（1）找国企或者央企，以应届毕业生的身份解决北京户口；（2）出国留学，用海外学位证换外企的北京户口指标。而住房方面现在只能总结一个字：穷。之前网上流行的一个段子：“最近用自己月薪3000攒下的1W，加老爸给的100W，刚刚好付了首付”，而现在更多的是“我真的还想再活五百年”。 &emsp;&emsp;痛定思痛，唯有创新。昨天刘洋向我介绍了好多小型的辅助工具：或微信埋雷抢红包，或批量自动添加好友，或接活帮小公司维护网站。。。在大企业中干活终究是个螺丝钉，还或许是备用的；而自己拥有能做主的事业才是真实掌握在手中的财富。虽说条条大路通罗马，但贫富差距也随之越来越大。 &emsp;&emsp;不觉间已经写了这么多，最后定几个小目标吧：&emsp;&emsp;&emsp;1. 依然坚持每天背单词看阅读打卡；&emsp;&emsp;&emsp;2. 从现在起每想到一个新鲜的idea立即做记录；&emsp;&emsp;&emsp;3. 每周为博客添一点色彩：不论是技术文章、生活随笔、或周边风景；&emsp;&emsp;&emsp;4. 每周看一篇关于人际交流或职场生活的文章；&emsp;&emsp;&emsp;5. 每月涉猎一个自己不熟悉的领域知识；&emsp;&emsp;&emsp;6. 找工作时，对比选择一个发展前景广并切实符合自己兴趣的公司。&emsp;&emsp;ps:其实还有一个大目标：我要减肥！！！/(ㄒoㄒ)/~","tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.killyliu.com/tags/随笔/"}]},{"title":"走在青春的路上","date":"2017-03-28T04:05:59.000Z","path":"2017/03/28/走在青春的路上/","text":"&emsp;&emsp;曾经幻想过自己将来的生活：或许能够成为企业中一名核心人物，运筹帷幄；亦或开一间小店平平淡淡过日子，岁月静好；也不排除辗转于世界各地，随遇而安。无论如何，我的青春我做主。 时光&emsp;&emsp;都说岁月无痕，只是不那么明显罢了。回想小时候，可能因为玩具脏了心情不爽就大哭一场，一切都是那么随性而为之。而当身上肩负的责任越来越多之后，正如邓紫棋歌中所唱，“你的笑只是你穿的保护色”，有时照顾他人的情绪要多于自己，越来越明白为什么人们总是怀念童年了。&emsp;&emsp;而人生没有如果，穿梭时光的机器属于大雄和叮当猫，但不属于我们。有时觉得时间一分一秒都是那么漫长，有时又不禁感慨时间流逝速度之快，并非是时光的魔法，一切均由心生。。。 经历&emsp;&emsp;“似水流年或沧海桑田，总是一眨眼消失在转身的那瞬间；沿途流逝的画面，时光如风掠过了双眼，阡陌稻田，远山云烟”。这些年，我们走过的路、看过的风景，都逐渐化作了经历，不知不觉间影响着为人与处事风格，或成熟稳重，或举步生风。&emsp;&emsp;懵懂时的好奇，悸动时的冲动，成熟时的稳重，都为成长的路上增添了许多色彩，而我也愈发喜欢这样的生活，这样的自己。经历过春夏秋冬的洗礼，体味过朝阳夕下的轮回，渐渐懂得了何时该温婉，何时该冷傲，何时该耀眼，何时该淡然。 梦想&emsp;&emsp;如果说杰伦的《梦想启动》充满积极正能量的话，范范《最初的梦想》则道明了路途中的坎坷与荆棘。古有“长风破浪会有时，直挂云帆济苍海”，今有“既然选择了远方，便只顾风雨兼程”。虽说三千繁华，弹指刹那，但掌握在自己手中的岁月不过百年而已。既已知晓，又如何任时光蹉跎？&emsp;&emsp;仰望星空与脚踏实地，永恒不变的主题，又有谁真正能问心无愧。合抱之木，生于毫末；九层之合，起于垒土；千里之行，始于足下。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://blog.killyliu.com/tags/随笔/"}]},{"title":"Hello World","date":"2017-03-27T13:11:17.000Z","path":"2017/03/27/hello-world/","text":"保留所有程序猿的Hello World情节~~ Hexo使用Create a new post1$ hexo new \"My New Post\" Run server1$ hexo server Generate static files1$ hexo generate Deploy to remote sites1$ hexo deploy","tags":[{"name":"技术","slug":"技术","permalink":"http://blog.killyliu.com/tags/技术/"},{"name":"随感","slug":"随感","permalink":"http://blog.killyliu.com/tags/随感/"}]}]